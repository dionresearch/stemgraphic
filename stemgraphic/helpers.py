""" helpers.py

Helper functions for stemgraphic.
"""
import matplotlib.tri as tri
from io import BytesIO
import numpy as np
import pandas as pd
import pickle
from warnings import warn

try:
    import dask.dataframe as dd
except ImportError:
    dd = False

try:
    import sixel
except ImportError:
    sixel = None


def jitter(data, scale):
    """Adds jitter to data, for display purpose

    :param data: numpy or pandas dataframe
    :param scale:
    :return:
    """
    return data + np.random.rand(len(data)) / (2 * scale)


def key_calc(stem, leaf, scale):
    """Calculates a value from a stem, a leaf and a scale.

    :param stem:
    :param leaf:
    :param scale:
    :return: calculated values
    """
    return (int(leaf) / 10 + int(stem)) * float(scale)


def legend(
    ax,
    x,
    y,
    asc,
    flip_axes,
    mirror,
    stem,
    leaf,
    scale,
    delimiter_color,
    aggregation=True,
    cur_font=None,
    display=10,
    pos="best",
    unit="",
):
    """ legend

    Builds a graphical legend for numerical stem-and-leaf plots.

    :param display:
    :param cur_font:
    :param ax:
    :param x:
    :param y:
    :param pos:
    :param asc:
    :param flip_axes:
    :param mirror:
    :param stem:
    :param leaf:
    :param scale:
    :param delimiter_color:
    :param unit:
    :param aggregation:
    """

    if pos is None:
        return
    aggr_fontsize = cur_font.get_size() - 2
    if (mirror and not flip_axes) or (flip_axes and not asc):
        ha = "right"
        formula = "{2}{1} =        x{0} = "
        offset = len(str(scale)) + 3.1 + (len(stem) + len(leaf)) / 1.7
        secondary = -2.5 if asc and flip_axes else -1.6
        key_text = "Key: leaf|stem{}".format("|aggr" if aggregation else "")
    else:
        ha = "left"
        formula = "  =         x{0} = {1}{2}"
        offset = 3.1
        secondary = 0.1
        key_text = "Key: {}stem|leaf".format("aggr|" if aggregation else "")
    start_at = (len(stem) * 2 + 11 + len(str(scale)) + len(leaf)) / 1.7
    if pos == "short":
        ax.text(
            x - start_at,
            y + 2,
            " x {}".format(scale),
            va="center",
            ha=ha,
            fontproperties=cur_font,
        )
    else:
        if aggregation:
            ax.text(
                x - start_at - 1,
                y + 2,
                key_text,
                va="center",
                ha=ha,
                fontproperties=cur_font,
            )
            ax.text(
                x - start_at - 2,
                y + 1,
                display,
                fontsize=aggr_fontsize - 2,
                va="center",
                ha=ha,
            )
        cur_font.set_weight("bold")
        ax.text(
            x - start_at - 1, y + 1, stem, va="center", ha=ha, fontproperties=cur_font
        )
        ax.text(
            x - start_at + (1 + len(leaf) + offset) / 1.7,
            y + 1,
            stem,
            va="center",
            ha=ha,
            fontproperties=cur_font,
        )
        cur_font.set_weight("normal")
        ax.text(
            x - start_at + (len(stem) + len(leaf)) / 1.7,
            y + 1,
            formula.format(scale, key_calc(stem, leaf, scale), unit),
            va="center",
            ha=ha,
            fontproperties=cur_font,
        )
        cur_font.set_style("italic")
        ax.text(
            x - start_at + 0.3,
            y + 1,
            leaf,
            bbox={"facecolor": "C0", "alpha": 0.15, "pad": 2},
            va="center",
            ha=ha,
            fontproperties=cur_font,
        )
        ax.text(
            x - start_at + (len(stem) + offset + len(leaf) + 0.6) / 1.7 + secondary,
            y + 1,
            "." + leaf,
            va="center",
            ha=ha,
            fontproperties=cur_font,
        )

        if flip_axes:
            ax.vlines(x - start_at, y + 0.5, y + 1.5, color=delimiter_color, alpha=0.7)
            if aggregation:
                ax.vlines(
                    x - start_at - 1, y + 0.5, y + 1.5, color=delimiter_color, alpha=0.7
                )
        else:
            ax.vlines(
                x - start_at + 0.1, y + 0.5, y + 1.5, color=delimiter_color, alpha=0.7
            )
            if aggregation:
                ax.vlines(
                    x - start_at - 1.1,
                    y + 0.5,
                    y + 1.5,
                    color=delimiter_color,
                    alpha=0.7,
                )


def min_max_count(x, column=0):
    """ min_max_count

    Handles min, max and count. This works on numpy, lists, pandas and dask dataframes.

    :param x: list, numpy array, series, pandas or dask dataframe
    :param column: future use
    :return: min, max and count
    """
    if dd and type(x) in (dd.core.DataFrame, dd.core.Series):
        omin, omax, count = dd.compute(x.min(), x.max(), x.count())
    elif type(x) in (pd.DataFrame, pd.Series):
        try:
            omin = x.min().values[0]
            omax = x.max().values[0]
        except AttributeError:
            omin = x.min()
            omax = x.max()
        count = len(x)
    else:
        omin = min(x)
        omax = max(x)
        count = len(x)

    return omin, omax, int(count)


def na_count(x, column=0):
    """ min_max_count

        Handles min, max and count. This works on numpy, lists, pandas and dask dataframes.

        :param x: list, numpy array, series, pandas or dask dataframe
        :param column: future use
        :return: all numpy nan count
        """
    val_missing = x.isnull().sum()
    return val_missing


def npy_save(path, array):
    if path[-4:] != ".npy":
        path += ".npy"
    with open(path, "wb+") as f:
        np.save(f, array, allow_pickle=False)
    return path


def npy_load(path):
    if path[-4:] != ".npy":
        warn("Not a numpy NPY file.")
        return None
    return np.load(path)


def pkl_save(path, array):
    if path[-4:] != ".pkl":
        path += ".pkl"
    with open(path, "wb+") as f:
        pickle.dump(array, f)
    return path


def pkl_load(path):
    if path[-4:] != ".pkl":
        warn("Not a PKL file.")
        return None
    with open(path, "rb") as f:
        matrix = pickle.load(f)
    return matrix


def percentile(data, alpha):
    """ percentile

    :param data: list, numpy array, time series or pandas dataframe
    :param alpha: between 0 and 0.5 proportion to select on each side of the distribution
    :return: the actual value at that percentile
    """
    n = sorted(data)
    low = int(round(alpha * len(data) + 0.5))
    high = int(round((1 - alpha) * len(data) + 0.5))
    return n[low - 1], n[high - 1]


def savefig(plt):
    """

    :return:
    """
    buf = BytesIO()
    plt.savefig(buf)
    buf.seek(0)
    if sixel is None:
        warn("No sixel module available. Please install pysixel")
    writer = sixel.SixelWriter()
    writer.draw(buf)


def stack_columns(row):
    """ stack_columns

    stack multiple columns into a single stacked value
    :param row: a row of letters
    :return: stacked string
    """
    row = row.dropna()
    stack = ""
    for i, col in row.iteritems():
        stack += str(i) * int(col)
    return stack


#: Typographical apostrophe - ex: Iâ€™m, lâ€™arbre
APOSTROPHE = "â€™"

#: Straight quote mark - ex: 'INCONCEIVABLE'
QUOTE = "'"

#: Double straight quote mark
DOUBLE_QUOTE = '"'

#: empty
EMPTY = b" "

#: for typesetting overlap
OVER = b"\xd6\xb1"

#: Characters to filter. Does a relatively good job on a majority of texts
#: '- ' and 'â€“' is to skip quotes in many plays and dialogues in books, especially French.
CHAR_FILTER = [
    "\t",
    "\n",
    "\\",
    "/",
    "`",
    "*",
    "_",
    "{",
    "}",
    "[",
    "]",
    "(",
    ")",
    "<",
    ">",
    "#",
    "=",
    "+",
    "- ",
    "â€“",
    ".",
    ";",
    ":",
    "!",
    "?",
    "|",
    "$",
    QUOTE,
    DOUBLE_QUOTE,
    "â€¦",
]


#: Similar purpose to CHAR_FILTER, ut keeps the period. The last word of each sentence will end with a '.'
#: Useful for manipulating the dataframe returned by the various visualizations and ngram_data,
#: to break down frequencies by sentence instead of the full text or list.
NO_PERIOD_FILTER = [
    "\t",
    "\n",
    "\\",
    "/",
    "`",
    "*",
    "_",
    "{",
    "}",
    "[",
    "]",
    "(",
    ")",
    "<",
    ">",
    "#",
    "=",
    "+",
    "- ",
    "â€“",
    ";",
    ":",
    "!",
    "?",
    "|",
    "$",
    QUOTE,
    DOUBLE_QUOTE,
]


#: Default definition of standard letters
#: remove_accent has to be called explicitely for any of these letters to match their
#: accented counterparts
LETTERS = "abcdefghijklmnopqrstuvwxyz"

#: List of non alpha characters. Temporary - I want to balance flexibility with convenience, but
#: still looking at options.
NON_ALPHA = [
    "-",
    "+",
    "/",
    "[",
    "]",
    "_",
    "Â£",
    "1",
    "2",
    "3",
    "4",
    "5",
    "6",
    "7",
    "8",
    "9",
    "0",
    "!",
    "@",
    "#",
    "$",
    "%",
    "^",
    "&",
    "*",
    "(",
    ")",
    ";",
    QUOTE,
    DOUBLE_QUOTE,
    APOSTROPHE,
    EMPTY,
    OVER,
    "?",
    "Â¡",
    "Â¿",  # spanish
    "Â«",
    "Â»",
    "â€œ",
    "â€",
    "-",
    "â€”",
]

#: Charset mappings
mapping = {
    "arabic": {
        "0": "Ù ",
        "1": "Ù¡",
        "2": "Ù¢",
        "3": "Ù£",
        "4": "Ù¤",
        "5": "Ù¥",
        "6": "Ù¦",
        "7": "Ù§",
        "8": "Ù¨",
        "9": "Ù©",
    },
    "arabic_r": {
        "0": "Ù ",
        "1": "Ù¡",
        "2": "Ù¢",
        "3": "Ù£",
        "4": "Ù¤",
        "5": "Ù¥",
        "6": "Ù¦",
        "7": "Ù§",
        "8": "Ù¨",
        "9": "Ù©",
    },
    "bold": {
        "0": "ðŸŽ",
        "1": "ðŸ",
        "2": "ðŸ",
        "3": "ðŸ‘",
        "4": "ðŸ’",
        "5": "ðŸ“",
        "6": "ðŸ”",
        "7": "ðŸ•",
        "8": "ðŸ–",
        "9": "ðŸ—",
    },
    "circled": {
        "0": "â“ª",
        "1": "â‘ ",
        "2": "â‘¡",
        "3": "â‘¢",
        "4": "â‘£",
        "5": "â‘¤",
        "6": "â‘¥",
        "7": "â‘¦",
        "8": "â‘§",
        "9": "â‘¨",
    },
    "default": {
        "0": "0",
        "1": "1",
        "2": "2",
        "3": "3",
        "4": "4",
        "5": "5",
        "6": "6",
        "7": "7",
        "8": "8",
        "9": "9",
    },
    "doublestruck": {
        "0": "ðŸ˜",
        "1": "ðŸ™",
        "2": "ðŸš",
        "3": "ðŸ›",
        "4": "ðŸœ",
        "5": "ðŸ",
        "6": "ðŸž",
        "7": "ðŸŸ",
        "8": "ðŸ ",
        "9": "ðŸ¡",
    },
    "fullwidth": {
        "0": "ï¼",
        "1": "ï¼‘",
        "2": "ï¼’",
        "3": "ï¼“",
        "4": "ï¼”",
        "5": "ï¼•",
        "6": "ï¼–",
        "7": "ï¼—",
        "8": "ï¼˜",
        "9": "ï¼™",
    },
    "gurmukhi": {
        "0": "à©¦",
        "1": "à©§",
        "2": "à©¨",
        "3": "à©©",
        "4": "à©ª",
        "5": "à©«",
        "6": "à©¬",
        "7": "à©­",
        "8": "à©®",
        "9": "à©¯",
    },
    "mono": {
        "0": "ðŸ¶",
        "1": "ðŸ·",
        "2": "ðŸ¸",
        "3": "ðŸ¹",
        "4": "ðŸº",
        "5": "ðŸ»",
        "6": "ðŸ¼",
        "7": "ðŸ½",
        "8": "ðŸ¾",
        "9": "ðŸ¿",
    },
    "nko": {
        "0": "ß€",
        "1": "ß",
        "2": "ß‚",
        "3": "ßƒ",
        "4": "ß„",
        "5": "ß…",
        "6": "ß†",
        "7": "ß‡",
        "8": "ßˆ",
        "9": "ß‰",
    },
    "rod": {
        "0": "â—¯",  # U+25EF LARGE CIRCLE
        "1": "ð©",
        "2": "ðª",
        "3": "ð«",
        "4": "ð¬",
        "5": "ð­",
        "6": "ð®",
        "7": "ð¯",
        "8": "ð°",
        "9": "ð±",
    },
    "roman": {
        "0": ".",  # No zero
        "1": "â… ",
        "2": "â…¡",
        "3": "â…¢",
        "4": "â…£",
        "5": "â…¤",
        "6": "â…¥",
        "7": "â…¦",
        "8": "â…§",
        "9": "â…¨",
    },
    "sans": {
        "0": "ðŸ¢",
        "1": "ðŸ£",
        "2": "ðŸ¤",
        "3": "ðŸ¥",
        "4": "ðŸ¦",
        "5": "ðŸ§",
        "6": "ðŸ¨",
        "7": "ðŸ©",
        "8": "ðŸª",
        "9": "ðŸ«",
    },
    "sansbold": {
        "0": "ðŸ¬",
        "1": "ðŸ­",
        "2": "ðŸ®",
        "3": "ðŸ¯",
        "4": "ðŸ°",
        "5": "ðŸ±",
        "6": "ðŸ²",
        "7": "ðŸ³",
        "8": "ðŸ´",
        "9": "ðŸµ",
    },
    "square": {
        "0": "ðŸžŒ",
        "1": "ðŸž",
        "2": "ï¿­",
        "3": "â¬›",
        "4": "ðŸž“",
        "5": "ðŸž’",
        "6": "ðŸž‘",
        "7": "ðŸž",
        "8": "ðŸž",
        "9": "ðŸžŽ",
    },
    "subscript": {
        "0": "â‚€",
        "1": "â‚",
        "2": "â‚‚",
        "3": "â‚ƒ",
        "4": "â‚„",
        "5": "â‚…",
        "6": "â‚†",
        "7": "â‚‡",
        "8": "â‚ˆ",
        "9": "â‚‰",
    },
    "tamil": {
        "0": "à¯¦",
        "1": "à¯§",
        "2": "à¯¨",
        "3": "à¯©",
        "4": "à¯ª",
        "5": "à¯«",
        "6": "à¯¬",
        "7": "à¯­",
        "8": "à¯®",
        "9": "à¯¯",
    },
}

alpha_mapping = {
    "boldsans": "ð—”ð—•ð—–ð——ð—˜ð—™ð—šð—›ð—œð—ð—žð—Ÿð— ð—¡ð—¢ð—£ð—¤ð—¥ð—¦ð—§ð—¨ð—©ð—ªð—«ð—¬ð—­ð—®ð—¯ð—°ð—±ð—²ð—³ð—´ð—µð—¶ð—·ð—¸ð—¹ð—ºð—»ð—¼ð—½ð—¾ð—¿ð˜€ð˜ð˜‚ð˜ƒð˜„ð˜…ð˜†ð˜‡",
    "bold": "ð€ðð‚ðƒð„ð…ð†ð‡ðˆð‰ðŠð‹ðŒððŽððð‘ð’ð“ð”ð•ð–ð—ð˜ð™ðšð›ðœððžðŸð ð¡ð¢ð£ð¤ð¥ð¦ð§ð¨ð©ðªð«ð¬ð­ð®ð¯ð°ð±ð²ð³",
    "circle": "â’¶â’·â’¸â’¹â’ºâ’»â’¼â’½â’¾â’¿â“€â“â“‚â“ƒâ“„â“…â“†â“‡â“ˆâ“‰â“Šâ“‹â“Œâ“â“Žâ“â“â“‘â“’â““â“”â“•â“–â“—â“˜â“™â“šâ“›â“œâ“â“žâ“Ÿâ“ â“¡â“¢â“£â“¤â“¥â“¦â“§â“¨â“©",
    "cursive": "ð’œðµð’žð’Ÿð¸ð¹ð’¢ð»ð¼ð’¥ð’¦ð¿ð‘€ð’©ð’ªð’«ð’¬ð‘…ð’®ð’¯ð’°ð’±ð’²ð’³ð’´ð’µð’¶ð’·ð’¸ð’¹ð‘’ð’»ð‘”ð’½ð’¾ð’¿ð“€ð“ð“‚ð“ƒð‘œð“…ð“†ð“‡ð“ˆð“‰ð“Šð“‹ð“Œð“ð“Žð“",
    "default": "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz",
    "doublestruck": "ð”¸ð”¹â„‚ð”»ð”¼ð”½ð”¾â„ð•€ð•ð•‚ð•ƒð•„â„•ð•†â„™â„šâ„ð•Šð•‹ð•Œð•ð•Žð•ð•â„¤ð•’ð•“ð•”ð••ð•–ð•—ð•˜ð•™ð•šð•›ð•œð•ð•žð•Ÿð• ð•¡ð•¢ð•£ð•¤ð•¥ð•¦ð•§ð•¨ð•©ð•ªð•«",
    "italicbold": "ð‘¨ð‘©ð‘ªð‘«ð‘¬ð‘­ð‘®ð‘¯ð‘°ð‘±ð‘²ð‘³ð‘´ð‘µð‘¶ð‘·ð‘¸ð‘¹ð‘ºð‘»ð‘¼ð‘½ð‘¾ð‘¿ð’€ð’ð’‚ð’ƒð’„ð’…ð’†ð’‡ð’ˆð’‰ð’Šð’‹ð’Œð’ð’Žð’ð’ð’‘ð’’ð’“ð’”ð’•ð’–ð’—ð’˜ð’™ð’šð’›",
    "italicboldsans": "ð˜¼ð˜½ð˜¾ð˜¿ð™€ð™ð™‚ð™ƒð™„ð™…ð™†ð™‡ð™ˆð™‰ð™Šð™‹ð™Œð™ð™Žð™ð™ð™‘ð™’ð™“ð™”ð™•ð™–ð™—ð™˜ð™™ð™šð™›ð™œð™ð™žð™Ÿð™ ð™¡ð™¢ð™£ð™¤ð™¥ð™¦ð™§ð™¨ð™©ð™ªð™«ð™¬ð™­ð™®ð™¯",
    "medieval": "ð”„ð”…â„­ð”‡ð”ˆð”‰ð”Šâ„Œâ„‘ð”ð”Žð”ð”ð”‘ð”’ð”“ð””â„œð”–ð”—ð”˜ð”™ð”šð”›ð”œâ„¨ð”žð”Ÿð” ð”¡ð”¢ð”£ð”¤ð”¥ð”¦ð”§ð”¨ð”©ð”ªð”«ð”¬ð”­ð”®ð”¯ð”°ð”±ð”²ð”³ð”´ð”µð”¶ð”·",
    "medievalbold": "ð•¬ð•­ð•®ð•¯ð•°ð•±ð•²ð•³ð•´ð•µð•¶ð•·ð•¸ð•¹ð•ºð•»ð•¼ð•½ð•¾ð•¿ð–€ð–ð–‚ð–ƒð–„ð–…ð–†ð–‡ð–ˆð–‰ð–Šð–‹ð–Œð–ð–Žð–ð–ð–‘ð–’ð–“ð–”ð–•ð––ð–—ð–˜ð–™ð–šð–›ð–œð–ð–žð–Ÿ",
    "square": "ðŸ„°ðŸ„±ðŸ„²ðŸ„³ðŸ„´ðŸ„µðŸ„¶ðŸ„·ðŸ„¸ðŸ„¹ðŸ„ºðŸ„»ðŸ„¼ðŸ„½ðŸ„¾ðŸ„¿ðŸ…€ðŸ…ðŸ…‚ðŸ…ƒðŸ…„ðŸ……ðŸ…†ðŸ…‡ðŸ…ˆðŸ…‰ðŸ„°ðŸ„±ðŸ„²ðŸ„³ðŸ„´ðŸ„µðŸ„¶ðŸ„·ðŸ„¸ðŸ„¹ðŸ„ºðŸ„»ðŸ„¼ðŸ„½ðŸ„¾ðŸ„¿ðŸ…€ðŸ…ðŸ…‚ðŸ…ƒðŸ…„ðŸ……ðŸ…†ðŸ…‡ðŸ…ˆðŸ…‰",
    "square_inverted": "ðŸ…°ðŸ…±ðŸ…²ðŸ…³ðŸ…´ðŸ…µðŸ…¶ðŸ…·ðŸ…¸ðŸ…¹ðŸ…ºðŸ…»ðŸ…¼ðŸ…½ðŸ…¾ðŸ…¿ðŸ†€ðŸ†ðŸ†‚ðŸ†ƒðŸ†„ðŸ†…ðŸ††ðŸ†‡ðŸ†ˆðŸ†‰ðŸ…°ðŸ…±ðŸ…²ðŸ…³ðŸ…´ðŸ…µðŸ…¶ðŸ…·ðŸ…¸ðŸ…¹ðŸ…ºðŸ…»ðŸ…¼ðŸ…½ðŸ…¾ðŸ…¿ðŸ†€ðŸ†ðŸ†‚ðŸ†ƒðŸ†„ðŸ†…ðŸ††ðŸ†‡ðŸ†ˆðŸ†‰",
    "typewriter": "ð™°ð™±ð™²ð™³ð™´ð™µð™¶ð™·ð™¸ð™¹ð™ºð™»ð™¼ð™½ð™¾ð™¿ðš€ðšðš‚ðšƒðš„ðš…ðš†ðš‡ðšˆðš‰ðšŠðš‹ðšŒðšðšŽðšðšðš‘ðš’ðš“ðš”ðš•ðš–ðš—ðš˜ðš™ðššðš›ðšœðšðšžðšŸðš ðš¡ðš¢ðš£",
}


def square_scale():
    return "ðŸžŒ ðŸž ï¿­ â¬› ðŸž“ ðŸž’ ðŸž‘ ðŸž ðŸž ðŸžŽ"


def available_charsets():
    return list(mapping.keys())


def available_alpha_charsets():
    return list(alpha_mapping.keys())


def translate_alpha_representation(text, charset=None):
    default = alpha_mapping["default"]
    lookup_charset = alpha_mapping[charset]

    lookup = dict(zip(default, lookup_charset))

    if charset == "arabic_r":
        if text[-1] != "|":
            text_string = text[::-1]
        else:
            text_string = text[text.find("|") - 1 :: -1] + text[-1:]
    else:
        text_string = text

    return "".join([lookup.get(c, c) for c in text_string])


def translate_representation(text, charset=None, index=None, zero_blank=None):
    lookup = mapping[charset]
    if charset == "arabic_r":
        if text[-1] != "|":
            text_string = text[::-1]
        else:
            text_string = text[text.find("|") - 1 :: -1] + text[-1:]
    else:
        text_string = text
    if index > 1 and zero_blank:
        text_string = text_string[:4] + text_string[4:].replace(" 0", "  ").replace(
            " nan", "    "
        )
    return "".join([lookup.get(c, c) for c in text_string])
